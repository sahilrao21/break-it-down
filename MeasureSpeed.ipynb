{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_movie = \"yuki_dance_full_vid.mov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def measure_speed(video):\n",
    "    count = 0\n",
    "    \n",
    "    vc = cv2.VideoCapture(video)\n",
    "    \n",
    "    total_frames = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vc.get(cv2.CAP_PROP_FPS))\n",
    "    duration = total_frames/fps # float\n",
    "\n",
    "    width  = int(vc.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vc.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    batches_diff = dict()\n",
    "    batches_frame_range = dict()\n",
    "    \n",
    "    curr_frame = 0\n",
    "    while vc.isOpened():\n",
    "        batches_frame_range[count] = [curr_frame, curr_frame]\n",
    "        \n",
    "        pixel_diffs = np.zeros(shape=(height, width), dtype=float)\n",
    "        prevFrame = None\n",
    "        \n",
    "        ret = True\n",
    "        i = 0\n",
    "        while i < fps and curr_frame < total_frames:\n",
    "            ret, frame = vc.read()\n",
    "            curr_frame += 1\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            grayFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            if not i:\n",
    "                prevFrame = grayFrame\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "        \n",
    "            pixel_diffs = np.add(pixel_diffs, np.abs(grayFrame.astype(float) - prevFrame.astype(float)))\n",
    "            prevFrame = grayFrame\n",
    "            i += 1\n",
    "        \n",
    "        batches_frame_range[count][-1] = curr_frame - 1\n",
    "        batches_diff[count] = np.sum(pixel_diffs)\n",
    "        count += 1\n",
    "            \n",
    "        # video finished\n",
    "        if not ret or curr_frame >= total_frames:\n",
    "            vc.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    return batches_frame_range, batches_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 91.84881174834554, 1: 51.584677803590566, 2: 47.87484489821501, 3: 39.242760345511996, 4: 33.951932332562684, 5: 73.28767068839969, 6: 83.13117623588414, 7: 64.93036050877971, 8: 85.71964017914607, 9: 60.703577366936045, 10: 52.71776800187116, 11: 49.972431501701216, 12: 34.27116658454022, 13: 34.566899763098746, 14: 52.33660412317235, 15: 43.248556757189206, 16: 46.8261112167063, 17: 55.451425560570335, 18: 52.17253331471687, 19: 63.508750670727636, 20: 80.8896735206156, 21: 56.512561269982335, 22: 35.35930669311019, 23: 32.46411753645115, 24: 45.87363618713823, 25: 65.06570200787701, 26: 64.4600098551738, 27: 51.793490795298254, 28: 34.01427267645424, 29: 22.0299527980999, 30: 33.75926929033067, 31: 66.15581264162991, 32: 83.43375453075564, 33: 73.86310531736055, 34: 58.29127083332272, 35: 51.17386075668779, 36: 51.73082432362882, 37: 56.05730014099931, 38: 46.675760196206625, 39: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# COMPUTING NORMALIZED BATCH DIFFERENCES\n",
    "batches_frame_range, batches_diff = measure_speed(input_movie)\n",
    "\n",
    "min_diff = float('inf')\n",
    "min_batch = float('inf')\n",
    "for k, v in batches_diff.items():\n",
    "    if min_diff > v:\n",
    "        min_batch = k\n",
    "        min_diff = v\n",
    "\n",
    "normalized_batches_diff = batches_diff.copy()\n",
    "for k in normalized_batches_diff.keys():\n",
    "    normalized_batches_diff[k] /= min_diff\n",
    "    \n",
    "\n",
    "print(normalized_batches_diff)\n",
    "# RUNNING MOVIE WITH SPEEDS ALTERED BY BATCH\n",
    "vc = cv2.VideoCapture(input_movie)\n",
    "output = cv2.VideoWriter('yuki_slowed_down.mp4', -1, 20.0, (568, 320))\n",
    "\n",
    "avg_weight = sum(normalized_batches_diff) / len(normalized_batches_diff)\n",
    "while vc.isOpened():\n",
    "    curr_batch = 0\n",
    "    while curr_batch < max(batches_frame_range.keys()) + 1:\n",
    "        for _ in range(batches_frame_range[curr_batch][1] - batches_frame_range[curr_batch][0] + 1):\n",
    "            ret, frame = vc.read()\n",
    "            cv2.imshow(\"\", frame)\n",
    "            \n",
    "            ## perhaps slow down worse ones even more??\n",
    "            cv2.waitKey(int(normalized_batches_diff[curr_batch] * 1.5))\n",
    "\n",
    "#             scaling_factor = np.log(normalized_batches_diff[curr_batch] / avg_weight) * 1.35\n",
    "#             if (scaling_factor <= 1):\n",
    "#                 scaling_factor = 1.65\n",
    "#             elif (scaling_factor > 3.2):\n",
    "#                 scaling_factor = 3.2\n",
    "#             cv2.waitKey(int(normalized_batches_diff[curr_batch] * scaling_factor))\n",
    "            \n",
    "        curr_batch += 1\n",
    "        \n",
    "    vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuki_weights = {0: 91.84881174834554, 1: 51.584677803590566, 2: 47.87484489821501, 3: 39.242760345511996, 4: 33.951932332562684, 5: 73.28767068839969, 6: 83.13117623588414, 7: 64.93036050877971, 8: 85.71964017914607, 9: 60.703577366936045, 10: 52.71776800187116, 11: 49.972431501701216, 12: 34.27116658454022, 13: 34.566899763098746, 14: 52.33660412317235, 15: 43.248556757189206, 16: 46.8261112167063, 17: 55.451425560570335, 18: 52.17253331471687, 19: 63.508750670727636, 20: 80.8896735206156, 21: 56.512561269982335, 22: 35.35930669311019, 23: 32.46411753645115, 24: 45.87363618713823, 25: 65.06570200787701, 26: 64.4600098551738, 27: 51.793490795298254, 28: 34.01427267645424, 29: 22.0299527980999, 30: 33.75926929033067, 31: 66.15581264162991, 32: 83.43375453075564, 33: 73.86310531736055, 34: 58.29127083332272, 35: 51.17386075668779, 36: 51.73082432362882, 37: 56.05730014099931, 38: 46.675760196206625, 39: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7.154956790828264,\n",
       " 1: 3.0197395861055276,\n",
       " 2: 2.7890182892553494,\n",
       " 3: 2.3181106130625144,\n",
       " 4: 2.0696917125436425,\n",
       " 5: 4.807341005621137,\n",
       " 6: 5.935998736284932,\n",
       " 7: 4.0192438890273845,\n",
       " 8: 6.274486011772089,\n",
       " 9: 3.6712695147770837,\n",
       " 10: 3.093943131535318,\n",
       " 11: 2.9172144154335022,\n",
       " 12: 2.083895682556901,\n",
       " 13: 2.0971409635707645,\n",
       " 14: 3.068780236720131,\n",
       " 15: 2.525841398000442,\n",
       " 16: 2.727052020844245,\n",
       " 17: 3.28055736253338,\n",
       " 18: 3.0580120486475577,\n",
       " 19: 3.8986746599321003,\n",
       " 20: 5.65767171454228,\n",
       " 21: 3.355992413051954,\n",
       " 22: 2.133047720185176,\n",
       " 23: 2.004759554703766,\n",
       " 24: 2.671967191780689,\n",
       " 25: 4.03091504135209,\n",
       " 26: 3.9789454372436093,\n",
       " 27: 3.0332792355472855,\n",
       " 28: 2.0724578484202687,\n",
       " 29: 1.6031628299787144,\n",
       " 30: 2.061166275211461,\n",
       " 31: 4.126164859869323,\n",
       " 32: 5.974604184668954,\n",
       " 33: 4.866974373663362,\n",
       " 34: 3.4863498724763087,\n",
       " 35: 2.9932779478888087,\n",
       " 36: 3.0292095136960957,\n",
       " 37: 3.3234182043154603,\n",
       " 38: 2.718281828459045,\n",
       " 39: 1.021655546827197}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yuki_weights = {0: 91.84881174834554, 1: 51.584677803590566, 2: 47.87484489821501, 3: 39.242760345511996, 4: 33.951932332562684, 5: 73.28767068839969, 6: 83.13117623588414, 7: 64.93036050877971, 8: 85.71964017914607, 9: 60.703577366936045, 10: 52.71776800187116, 11: 49.972431501701216, 12: 34.27116658454022, 13: 34.566899763098746, 14: 52.33660412317235, 15: 43.248556757189206, 16: 46.8261112167063, 17: 55.451425560570335, 18: 52.17253331471687, 19: 63.508750670727636, 20: 80.8896735206156, 21: 56.512561269982335, 22: 35.35930669311019, 23: 32.46411753645115, 24: 45.87363618713823, 25: 65.06570200787701, 26: 64.4600098551738, 27: 51.793490795298254, 28: 34.01427267645424, 29: 22.0299527980999, 30: 33.75926929033067, 31: 66.15581264162991, 32: 83.43375453075564, 33: 73.86310531736055, 34: 58.29127083332272, 35: 51.17386075668779, 36: 51.73082432362882, 37: 56.05730014099931, 38: 46.675760196206625, 39: 1.0}\n",
    "min_weight = 46.675760196206625\n",
    "for k in yuki_weights.keys():\n",
    "    yuki_weights[k] /= 46.675760196206625\n",
    "    yuki_weights[k] = np.e ** yuki_weights[k]\n",
    "\n",
    "yuki_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(video):\n",
    "    count = 0\n",
    "    \n",
    "    vc = cv2.VideoCapture(video)\n",
    "    \n",
    "    total_frames = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vc.get(cv2.CAP_PROP_FPS))\n",
    "    duration = total_frames/fps # float\n",
    "    \n",
    "    width  = int(vc.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vc.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    batches_diff = dict()\n",
    "    batches_frame_range = dict()\n",
    "    \n",
    "    curr_frame = 0\n",
    "    while vc.isOpened():\n",
    "        batches_frame_range[count] = [curr_frame, curr_frame]\n",
    "        \n",
    "        pixel_diffs = np.zeros(shape=(height, width), dtype=float)\n",
    "        prevFrame = None\n",
    "        \n",
    "        ret = True\n",
    "        i = 0\n",
    "        while i < fps and curr_frame < total_frames:\n",
    "            ret, frame = vc.read()\n",
    "            curr_frame += 1\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            grayFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            if not i:\n",
    "                prevFrame = grayFrame\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "        \n",
    "            pixel_diffs = np.add(pixel_diffs, np.abs(grayFrame.astype(float) - prevFrame.astype(float)))\n",
    "#             print(\"prevFrame\", prevFrame[10][:20])\n",
    "#             print(\"grayFrame\", grayFrame[10][:20])\n",
    "#             print(\"pixel_diffs\", pixel_diffs[10][:20])\n",
    "\n",
    "            prevFrame = grayFrame\n",
    "            i += 1\n",
    "        \n",
    "        batches_frame_range[count][-1] = curr_frame - 1\n",
    "        batches_diff[count] = np.sum(pixel_diffs)\n",
    "        count += 1\n",
    "            \n",
    "        # video finished\n",
    "        if not ret or curr_frame >= total_frames:\n",
    "            vc.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    \n",
    "#     print(fps)\n",
    "#     print(total_frames)\n",
    "#     print(\"outside while loop!\\n\")\n",
    "#     print(batches_diff.items())\n",
    "#     print(\"\\n\", batches_frame_range.items())\n",
    "    return batches_frame_range, batches_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4667023.0, 1: 6368225.0, 2: 3158268.0, 3: 3371236.0, 4: 2858192.0, 5: 3043888.0, 6: 2081803.0, 7: 1562428.0, 8: 906721.0, 9: 2280888.0, 10: 1332244.0, 11: 409456.0} \n",
      "\n",
      "{0: 11.398106267828533, 1: 15.55289213004572, 2: 7.713326950881169, 3: 8.233451213317181, 4: 6.9804618811300845, 5: 7.433980696338557, 6: 5.08431430971826, 7: 3.815862998710484, 8: 2.214452834981048, 9: 5.570532609120394, 10: 3.253692704466414, 11: 1.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batches_frame_range, batches_diff = measure_speed(input_movie)\n",
    "print(batches_diff, \"\\n\")\n",
    "\n",
    "min_diff = float('inf')\n",
    "min_batch = float('inf')\n",
    "for k, v in batches_diff.items():\n",
    "    if min_diff > v:\n",
    "        min_batch = k\n",
    "        min_diff = v\n",
    "\n",
    "normalized_batches_diff = batches_diff.copy()\n",
    "for k in normalized_batches_diff.keys():\n",
    "    normalized_batches_diff[k] /= min_diff\n",
    "    \n",
    "print(normalized_batches_diff, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "vc = cv2.VideoCapture(\"highdef.mov\")\n",
    "output = cv2.VideoWriter('relative_slowdown.mp4', -1, 20.0, (568, 320))\n",
    "\n",
    "while vc.isOpened():\n",
    "    curr_batch = 0\n",
    "    while curr_batch < max(batches_frame_range.keys()) + 1:\n",
    "        for _ in range(batches_frame_range[curr_batch][1] - batches_frame_range[curr_batch][0] + 1):\n",
    "            ret, frame = vc.read()\n",
    "            cv2.imshow(\"\", frame)\n",
    "#             cv2.waitKey(math.ceil(1 * normalized_batches_diff[curr_batch]))\n",
    "            cv2.waitKey(100)\n",
    "            \n",
    "        curr_batch += 1\n",
    "        \n",
    "    vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying code from here: https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and OpenCV\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Read input video\n",
    "input_vid = \"highdef.mov\"\n",
    "cap = cv2.VideoCapture(input_vid);\n",
    "\n",
    "# Get frame count\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "\n",
    "# Get width and height of video stream\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec for output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "# Set up output video\n",
    "output_vid = \"video_out.mp4\"\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "out = cv2.VideoWriter(output_vid, fourcc, fps, (w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first frame\n",
    "_, prev = cap.read() \n",
    "\n",
    "# Convert frame to grayscale\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0bfb08843cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Extract traslation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Pre-define transformation-store array\n",
    "transforms = np.zeros((n_frames-1, 3), np.float32) \n",
    "\n",
    "for i in range(n_frames-2):\n",
    "    # Detect feature points in previous frame\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, \\\n",
    "                                     maxCorners=200, \\\n",
    "                                     qualityLevel=0.01, \\\n",
    "                                     minDistance=30, \\\n",
    "                                     blockSize=3)\n",
    "    \n",
    "    # Read next frame\n",
    "    success, curr = cap.read() \n",
    "    if not success: \n",
    "        break \n",
    "    \n",
    "    # Convert to grayscale\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Calculate optical flow (i.e. track feature points)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None) \n",
    "    \n",
    "    # Sanity check\n",
    "    assert prev_pts.shape == curr_pts.shape \n",
    "    \n",
    "    # Filter only valid points\n",
    "    idx = np.where(status==1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    "\n",
    "    #Find transformation matrix\n",
    "    m = cv2.estimateAffine2D(prev_pts, curr_pts) #will only work with OpenCV-3 or less\n",
    "   \n",
    "    # Extract traslation\n",
    "    dx = m[0,2]\n",
    "    dy = m[1,2]\n",
    "\n",
    "    # Extract rotation angle\n",
    "    da = np.arctan2(m[1,0], m[0,0])\n",
    "   \n",
    "    # Store transformation\n",
    "    transforms[i] = [dx,dy,da]\n",
    "   \n",
    "    # Move to next frame\n",
    "    prev_gray = curr_gray\n",
    "\n",
    "    print(\"Frame: \" + str(i) +  \"/\" + str(n_frames) + \" -  Tracked points : \" + str(len(prev_pts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1-openvino'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
